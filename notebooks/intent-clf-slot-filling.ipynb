{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchtext\n",
    "from torchtext import data, datasets\n",
    "from transformers import BertConfig, BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_type = 'bert-base-uncased'\n",
    "max_tok_len = 128\n",
    "cls_token, sep_token = '[CLS]', '[SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():    \n",
    "    max_len, sents = 0, []\n",
    "    with open('data/atis.sentences.train.csv', 'r') as f:\n",
    "        raw = f.read().splitlines()        \n",
    "        for sent in raw:\n",
    "            splits = sent.split(',')\n",
    "            max_len = max(max_len, len(splits))\n",
    "            # Wordpiece tokenizer (downstream) doesn't play well with the\n",
    "            # default BertTokenizer's cls_token and sep_token parameters\n",
    "            splits = [cls_token] + splits[1:-1] + [sep_token]\n",
    "            sents.append(' '.join(splits))    \n",
    "    with open('data/atis.slots.train.csv', 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        slots = [x.split(',') for x in raw]\n",
    "    with open('data/atis.intent.train.csv', 'r') as f:\n",
    "        intents = f.read().splitlines()\n",
    "    # Replace BOS slot label with intent\n",
    "    slots = [[intents[idx]] + x[1:] for idx, x in enumerate(slots)]\n",
    "    # Lower-case all slot labels - text lower casing handled by Bert tokenizer\n",
    "    lower_slots = list(map(lambda x: [y.lower() for y in x], slots))\n",
    "    del intents\n",
    "    return sents, lower_slots, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_idxes(tokenizer, sents, max_tok_len):\n",
    "    num_sents = len(sents)\n",
    "    # Choosing pad-token idx to be 0 by default\n",
    "    tokens = np.zeros((num_sents, max_tok_len), dtype=np.int)\n",
    "    relevant_tok_mask = np.zeros((num_sents, max_tok_len), dtype=np.int)\n",
    "    attn_mask = np.zeros((num_sents, max_tok_len), dtype=np.int)\n",
    "    \n",
    "    # Use wordpiece tokenizer, and maintain idxes of those tokens that are useful\n",
    "    # In this case, that is the CLS-token, and the first subword token for every word\n",
    "    for idx, sent in enumerate(sents):\n",
    "        sent_toks = tokenizer.tokenize(sent)\n",
    "        attn_mask[idx, :len(sent_toks)] = 1\n",
    "        tokens[idx, :len(sent_toks)] = tokenizer.convert_tokens_to_ids(sent_toks)\n",
    "        relevant_tok_mask[idx, :len(sent_toks)] = [0 \n",
    "                                                   if (tok.startswith('#')\n",
    "                                                       or tok in ('EOS')) is True\n",
    "                                                   else 1\n",
    "                                                   for idx, tok in enumerate(sent_toks)]\n",
    "    return tokens, relevant_tok_mask, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(labels):\n",
    "    label_map = {l.lower():i for i, l in enumerate(set(itertools.chain(*labels)))}\n",
    "    idxes = list()\n",
    "    for label_list in labels:\n",
    "        idxes.append([label_map[l.lower()] for l in label_list])\n",
    "    return label_map, idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, slots, max_sent_token_len = load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1224 03:03:24.523384 140286189291328 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sduddu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Wordpiece tokenizer doesn't respect cls_token/sep_token supplied here\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_type,\n",
    "                                          do_basic_tokenize=False,\n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: This isn't entirely correct, since max_tok_len is for the *wordpiece* tokenizer len\n",
    "assert (max_sent_token_len <= max_tok_len), \"Max naive token len greater than max tok len\"\n",
    "tokens, rel_tok_mask, attn_mask = conv_to_idxes(tokenizer, sents, max_tok_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "slot_map, slot_ids = to_categorical(slots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
