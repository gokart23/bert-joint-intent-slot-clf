{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1224 01:09:57.401601 139955959142208 file_utils.py:35] PyTorch version 1.0.0 available.\n",
      "W1224 01:09:58.151705 139955959142208 __init__.py:28] To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import data, datasets\n",
    "from transformers import AdamW, BertConfig, BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_type = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True,\n",
    "                  tokenize=lambda x: x.split(','),\n",
    "                  preprocessing=lambda x: ['[CLS]'] + x[1:-1] + ['[SEP]'])\n",
    "SLOTS = data.Field(sequential=True, lower=True,tokenize=lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointSlotIntent(data.Dataset):\n",
    "    def __init__(self, slot_field):\n",
    "        self.__slots = data.TabularDataset(\"data/atis.slots.train.csv\", format='tsv',\n",
    "                                            fields=[('slots', slot_field)])\n",
    "        __intents = data.TabularDataset(\"data/atis.intent.train.csv\", format='tsv',\n",
    "                                              fields=[('intent', data.Field(lower=True))])\n",
    "        for intent, slot in zip(__intents, self.__slots):\n",
    "            slot.slots[0] = intent.intent[0]\n",
    "        del __intents\n",
    "        slot_field.build_vocab(self.__slots)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.__slots)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        return self.__slots[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = data.TabularDataset(\"data/atis.sentences.train.csv\", format=\"tsv\",\n",
    "                            fields=[('sent', TEXT)])\n",
    "TEXT.build_vocab(sents)\n",
    "joint = JointSlotIntent(SLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1224 01:16:51.775768 139955959142208 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/sduddu/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_type, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 20 [['[CLS]'], ['i'], ['want'], ['to'], ['fly'], ['from'], ['boston'], ['at'], ['83', '##8'], ['am'], ['and'], ['arrive'], ['in'], ['denver'], ['at'], ['111', '##0'], ['in'], ['the'], ['morning'], ['[SEP]']]\n",
      "\t 20 [101, 1045, 2215, 2000, 4875, 2013, 3731, 2012, 100, 2572, 1998, 7180, 1999, 7573, 2012, 100, 1999, 1996, 2851, 102]\n",
      "\t ['[CLS]', 'i', 'want', 'to', 'fly', 'from', 'boston', 'at', '83', '##8', 'am', 'and', 'arrive', 'in', 'denver', 'at', '111', '##0', 'in', 'the', 'morning', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "x = [tokenizer.wordpiece_tokenizer.tokenize(x) for x in sents[0].sent]\n",
    "y = tokenizer.convert_tokens_to_ids(sents[0].sent)\n",
    "print (\"\\t\", len(x), x)\n",
    "print (\"\\t\", len(y), y)\n",
    "print (\"\\t\", tokenizer.tokenize(' '.join(sents[0].sent)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
